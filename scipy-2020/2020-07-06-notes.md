# Scipy Notes: Day 1

## Tutorial: Bayesian Data Analysis

**Summary:** Learn about a few fundamental distribution types: binomial,
poisson, exponential, gaussian. Refresher on Bayes' theorem, Bayesian inference,
and learn probabilistic programming with PyMC3.

**Links:**

- https://github.com/ericmjl/bayesian-stats-modelling-tutorial/tree/master/notebooks/SciPy-2020
- https://ericmjl.github.io/essays-on-data-science/machine-learning/computational-bayesian-stats/
- http://www.math.wm.edu/~leemis/2008amstat.pdf
- https://twiecki.io/blog/2013/08/27/bayesian-glms-2
- https://pdfs.semanticscholar.org/dea6/0927efbd1f284b4132eae3461ea7ce0fb62a.pdf


## Scipy Tools Plenary Session: Maintainers Track

- `napari`
- `sympy`
- `holoviz`
- `numpy`
- analyze and visualize volumetric data: https://github.com/yt-project

**Links:**

- https://docs.rapids.ai/api/cudf


## Machine Learning Track

[youTube playlist](https://www.youtube.com/playlist?list=PLYx7XA2nY5GejOB1lsvriFeMytD1-VS1B)

### dabl: AutoML with a Human in the Loop

[video](https://www.youtube.com/watch?v=TQnxH90PqFc)

A library to get quick data visualizations, models, and explanations to
establish ML baselines quickly. Showcases some nice visualizations to look at
during the ML-modeling workflow.

This package looks like it's still early on in its development but is
promising for automating key parts of the machine learning workflow, at least
at the early prototype phase of a project.

### Forecasting Solar Flares

[video](https://www.youtube.com/watch?v=iZfwQWf75e8)

Using machine learning to forecast the formation of solar flares using
non-traditional feature sets. Highlighted 5 different studies using ML to
study/ understand solar flares using image data directly.

This talk gave a good overview into a field that I'm unfamiliar with, but
I would have liked it more if the presenter (a) dove into more details on each
study re: methodology, results, next steps, (b) connected the different studies
at a conceptual level and provide a bigger picture.

### geomstats: Geometric Machine Learning

[video](https://www.youtube.com/watch?v=Ju-Wsd84uG0)

Problem: How to generalize points, vectors, addition, subtraction on manifolds,
e.g. sphere, social networks, brain connectome

Analogies
- point -> base point
- vectors -> tangent vectors
- addition -> exponential
- subtraction -> logarithm

Machine learning on a vector space. `geomstats` provides utility to
convert points on a tangent space to a vector space with `ToTangentSpace` in
order to use `sklearn` supervised learning algorithms.
- Unsupervised learning can be done with `geomstats.learning`, e.g. PCA
- Supervised learning algorithms also provided by `geomstats.learning`, e.g.
  K-means

Good presentation featuring a useful-looking package.

### GPU-accelerated data analytics in Python

[video](https://www.youtube.com/watch?v=RniE3v8cTWs)

Problem: Bottleneck using GPU. When connecting two applications together,
somethings they don't share the same data format, resulting a lot of
copy-convert steps, which is inefficient.

RAPIDS: Features HPC open-source software on Python:
- Dask: distributed workloads on numpy, pandas, arbitrary python
- OpenUCX: hardware-accelerated communications
- cuDF: pandas-like dataframe that run on GPUs
  - Faster ETL!
  - Better string support (regex supports)
- cuML: GPU-accelerated sklearn-like ML library
- cuGraph: Networkx-like library for graph sanalytics
- cuXfilter: GPU-accelerated cross-filtering
- pyViz integration: python visualizations
- plotly dash
- BlazingSQL: GPU-accelearted SQL engine

Basically this is Nvidia providing software/hardware layers that integrates
with or re-implements popular open-source libraries for ease of onboarding new
users onto their RAPIDSAI platform.

### JAX: Accelerated Machine Learning Research

[video](https://www.youtube.com/watch?v=z-WSrQDXkuM)

"Implementing performant and scalable deep learning in Python" using numpy-like
interface:
- GPU/TPU support
- Autodiff
- Compilation/fusing of operations
- Parallelization of data and computation

Just `import jax.numpy as np`, for numpy API.
- `jax.grad` for autodiff
- `jax.vmap` to vectorize operations: `vmap(fn)(x)`
- `jax.pmap` to parallelize operations
- `jax.jit` for jit compilation to compile functions that use `jax.numpy`
  operations to optimize function for GPU/TPU.

This was a nice presentation, dissecting how `jax` compiles functions that use
`jax.numpy` into [XLA](https://www.tensorflow.org/xla) and demoing the main
functionality of the library. Seems very flexible for deep learning, especially
if you don't want to rely on higher-level abstractions offered by libraries
like pytorch/tensorflow.

### Learning from Evolving Data Streams

[video](https://www.youtube.com/watch?v=sw85SCv847Y)

`scikit-multiflow`: package for data streams

Contrasts batch data ML to streaming data ML.
- Stream learning: assume data is infinit, maintain models in online fashion
- Unbounded training sets, incorporate data on the fly, be efficient resource
  wise, detect changes and adapt

Requirements:
- Process one sample at a time, inspect only once
- Use limited amount of memory and time
- Always ready to predict

Very Fast Decision Tree (a.k.a. Hoeffding Tree)
- A type of decision tree that can handle small-sample updates 
- Classic decision trees need large batch of data
- Hoeffding Tree allows for choosing when you've seen enough data to create a
  split node

Concept Drift: changes in data
- detecting distributional change
- avoid marking outliers as change
- XGBoost (batch) vs. Adaptive Random Forest (streaming)
  - streaming model recovers quickly from concept drift (in simulated dataset)

Evaluation
- Holdout: an independent test set in currently available data in the stream at
  regular time intervals.
- Prequential: test the new instance, then train on model
  - Advantage: train on all of the data

### Machine Learning Model Serving

[video](https://www.youtube.com/watch?v=rFZhwsSxZ_Q)

Showcase `ray[serve]` as a solution for model serving

Requirements:
- Prediction delivery
- Multiplex models
- Scaling replicas
- Batch requests
- Model versioning
  - zero downtime deploying
  - gradual rollout/rollback
  - A/B testing
- Handle preprocessing/postprocessing logic
- Performance and Cost

Two approaches:
1. Embed model in the web server (Flask)
2. Offload prediction to external specialized service

`ray[serve]`:
- scalability
- end-to-end control
- enable complex pipelines
- programmability and observability

Nice overview of requirements for model serving and the main approaches
that are currently used to fulfill these requirements. Would like to look into
how this compares with solutions offered by GCP and AWS Sagemaker.

### Optimizing Humans and Machines to Advance Science

[video](https://www.youtube.com/watch?v=ENOf0IZDla8)

Talk about feature selection in machine learning. How to go about choosing
methods for a model for a particular project.

Feature Selection Benefits:
- Shorter training time
- Prevent overfitting
- Avoid overfitting and increase accuracy
- Increased interpretability of model

Types:
- Filter methods: extract features from data
  - E.g. Mutual information, ANOVA f-score, Fast correlation-based Filter
  - Pros: fast and simple to apply
  - Cons: does not account for interactions between features
- Wrapper methods: rank features by usefulness, select `n` top features.
  - E.g. forward selection, backward elimination
  - Pros: detect interactions between variables
  - Cons: computationally expensive, overfitting risk
- Embedded methods: combines feature selection and model training into one
  - E.g. regularization or tree-based methods
  - Pros: detect interactions between variables, relatively fast
  - Cons: model dependent, if classifier/regressor is changed the features may
    not work

How to pick?
- Is my model fitting process computationally expensive?
- Is the model I am trying to learn highly complex?
- Does my model admit embedded feature selection?

Case Study: Develop model that can target feature relations hypothesis
generation and predict properties of biomass derived molecules for aviation
fuels. Feature set composed of 1,800 molecular descriptors.


### Pandera: Statistical Data Validation of Pandas DataFrames

[video](https://www.youtube.com/watch?v=PxTLD-ueNd4)

This was my presentation on statistical data validation of pandas dataframes.

> Data with the wrong type, or with invalid values, can cause hard-to-locate
> downstream modeling errors. Pandera lets you define validation schemas to help
> you catch these error sources early.

- [@dillonniederhut](https://twitter.com/dillonniederhut/status/1283221074059108352)

### Ray: A System for Scalable Python and ML

https://www.youtube.com/watch?v=XIu8ZF7RSkw

Describes `ray`, a package for ML distributed computing.
- Featurization
- Streaming
- Hyperparameter tuning
- Model training
- Simulation
- Model serving

Why build a new distributed system?
- HPC, Deep Learning, Data Processing, Application Microservices are no longer
  isolated workloads. `ray` integrates these workloads.

`@ray.remote` decorator uses the python `futures` objects to parallelize
arbitrary python functions, specifying resource requirements e.g.
`@ray.remote(num_gpus=1)`

Highlights:
- 10x improved throughput improvement
- Auto-launch clusters on AWS, Azure, GCP
- Dashboard for debugging ray applications
