# Scipy Notes: Day 2

## Tutorial: Parallel and Distributed Computing in Python with Dask

https://github.com/dask/dask-tutorial


## Scipy Tools Plenary Session

- `scikit-learn`
- `matplotlib`: check out `subplot_mosiac`
- `bokeh`

## High Performance Python Track

[YouTube Playlist](https://www.youtube.com/playlist?list=PLYx7XA2nY5GfY4WWJjG5cQZDc7DIUmn6Z)

### Analyzing the Performance of Python Applications Using Multiple Levels of Parallelism

[video](https://www.youtube.com/watch?v=hTmXpvws52M)

HPC has typically been implemented in lower-level languages like C, C++,
Fortran. How to do performance analysis on high-level interfaces:
Numpy/Cupy/Tensorflow?

This talk is about `Score-P` performance measurement for various environments,
including python applications, install with:  `pip install scorep`.

How-to: profile `CuPy` application. Seems simple to profile code via:

```
import scorep as sp

with sp.user.region("init"):
   # configuration

sp.user.region_begin("compute")
# your e.g. numpy/tensorflow/etc. code

sp.user.region_end("compute")
```

Track memory consumption, runtime, etc. grouped by function calls, `region`s

### Arkouda: Terascale Data Science at Interactive Rates

[video](https://www.youtube.com/watch?v=Rqak62zNOxE)

Motivation: have HPC-scale problem, have access to HPC systems

- `Python3` client <-> `Chapel` server model
- Offers numpy/pandas-like API for filtering, transforms, etc.
- Completes tera-scale operations at interactive rates "within the human
  thought loop"
- Data science demands scaling. Often, even data exploration requires using the
  entire dataset

`Chapel`: a modern parallel programming language. `Arkouda` is a python
interface to `Chapel`, like `numpy` is an interface to lower-level language.
- `Chapel` features parallelism as a first-class citizen. Can scale from single
  node to super computer.


### Awkward Array: Manipulating JSON like Data with NumPy like Idioms

[video](https://www.youtube.com/watch?v=WlnUF3LRBj4)

Awkward array enables numpy-like idioms on json data structure. Great for
analyzing arbitrarily nested data structures.

- Grounded in motivation for particle physics analysis.
- Neat handling of broadcasting for operations on variable length lists, and
  generalization of numpy ndarray.
- Provides support for `numba` jit compilation.
- Overview of columnar data structures

### Boost-histogram: High-Performance Histograms as Objects

[video](https://www.youtube.com/watch?v=ERraTfHkPd0)

N-dimensional histograms as objects: continuous data into discrete bins.
Boost-histogram gives you an object that you can manipulate and plot.

- python bindings on C++ library
- can re-bin quickly
- can slice histogram by bin or data coordinate
- is fast! 10x over numpy binning
- easy to convert to/from numpy

### Bringing GPU support to Datashader: A RAPIDS case study

[video](https://www.youtube.com/watch?v=9s_ZfDG9Aj0)

GPU support for generating visualizations of large datasets to render: points,
lines, meshes, networks. Performs clever aggregation of data for efficiency
and handling aesthetics of visualizations.

Challenges:
- copy-over of CPU -> GPU -> CPU
- code reuse: GPU programming using C++ on CUDA

Solution
- Avoiding data transfer with RAPIDS project: cuDF, cuML, cuGraph, CuPy, Numba
  for CUDA.
- Can get 1-2 orders of magnitude speed up of visualization of large datasets
  (300 million data points)

### cuSignal - GPU Accelerating SciPy Signal with Numba and CuPy

[video](https://www.youtube.com/watch?v=yYlX2bbdXDk)

Another NVIDIA-platform library for signal processing that copies the
`scipy.signal` module as drop-in replacement, but, wait for it... it's GPU
accelerated!

### Fluctuation X ray Scattering Real-Time App

Particle acceleration! Study structure and function of macromolecules by
hitting them with very fast X rays.

### Interactive Supercomputing with Jupyter at NERSC

NERSC (National Energy Research Scientific Computing): support high-performance
computing through Jupyter. Goes through how Jupyter is deployed and how their
team supports users in difference scientific backgrounds.

### Leading Magnetic Fusion Energy Science into the Big and Fast Data Lane

Accelerate research on magnetic fusion energy. Use Python to accelerate data
analysis workflow. Motivation: Fusion energy promises unlimited clean energy.
Fusion experiments are large, with international collaborations. Experiments
are done every 30 minutes, to allow for cool-down phase for the hardware that
produces the data. In this cooldown period, they would like to analyze the data
generated by that run so that the scientist can optimize the experimental
setup.

### Legate Numpy: Accelerated and Distributed Array Computing

Drop-in replacement of numpy that allows you to scale from:
- single GPU
- multiple GPUs on single node
- multiple GPUs on multiple nodes
- GPUs on a supercomputer

Interoperability with `numpy`, built on top of `legion` (HPC platform).
